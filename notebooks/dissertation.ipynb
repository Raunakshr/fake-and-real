{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dissertation Notebook\n",
        "End-to-end pipeline for fake news detection using classical and deep learning models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fake = pd.read_csv('data/Fake.csv.zip')\n",
        "true = pd.read_csv('data/True.csv.zip')\n",
        "fake['label'] = 0\n",
        "true['label'] = 1\n",
        "df = pd.concat([fake, true]).reset_index(drop=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.info()\n",
        "df['label'].value_counts()\n",
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
        "    words = nltk.word_tokenize(text.lower())\n",
        "    words = [lemmatizer.lemmatize(w) for w in words if w not in stop_words and len(w) > 2]\n",
        "    return ' '.join(words)\n",
        "\n",
        "df['clean_text'] = df['text'].apply(clean_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Extraction and Classical Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['clean_text'], df['label'], test_size=0.2, random_state=42)\n",
        "clf_pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(max_features=5000)),\n",
        "    ('lr', LogisticRegression(max_iter=1000))\n",
        "])\n",
        "clf_pipeline.fit(X_train, y_train)\n",
        "clf_preds = clf_pipeline.predict(X_test)\n",
        "print(classification_report(y_test, clf_preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deep Learning Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "train_sequences = tokenizer.texts_to_sequences(X_train)\n",
        "test_sequences = tokenizer.texts_to_sequences(X_test)\n",
        "max_len = 100\n",
        "X_train_seq = pad_sequences(train_sequences, maxlen=max_len)\n",
        "X_test_seq = pad_sequences(test_sequences, maxlen=max_len)\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=64, input_length=max_len),\n",
        "    LSTM(64),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train_seq, y_train, epochs=2, validation_split=0.2)\n",
        "deep_preds = (model.predict(X_test_seq) > 0.5).astype('int32')\n",
        "print(classification_report(y_test, deep_preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Export Pipelines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib\n",
        "clf_pipeline.fit(df['clean_text'], df['label'])\n",
        "joblib.dump(clf_pipeline, 'models/log_reg_pipeline.joblib')\n",
        "model.save('models/lstm_model.keras')\n",
        "print('Pipelines saved!')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}